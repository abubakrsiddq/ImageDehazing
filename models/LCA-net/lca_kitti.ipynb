{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lca_kitti.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTEmbSvrCrRmbRf1C4KjSu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abubakrsiddq/ImageDehazing/blob/main/models/LCA-net/lca_kitti.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMQczzV5fF77"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "import time\n",
        "import datetime\n",
        "import keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import mean_squared_error\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujd3OBdXfHjp"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XirkTeahfsfG"
      },
      "source": [
        "## Data loader Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV1aRRAWfJMB"
      },
      "source": [
        "def load_image(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    \n",
        "    img = tf.io.decode_png(img, channels = 3)\n",
        "    img = tf.image.resize(img, size = (352 , 1216), antialias = True)\n",
        "    img = img / 255.0\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jNDnF4-fLSA"
      },
      "source": [
        "def dataset_preposses(orig_img_path='/content/drive/MyDrive/generated_dataset/trans',hazy_img_path='/content/drive/MyDrive/generated_dataset/hazy',percentage=0.1,validation_size=200,test_size=64,seed_val=200):\n",
        "    train_img = []\n",
        "    val_img = []\n",
        "    random.seed=seed_val\n",
        "    orig_img = glob.glob(orig_img_path + '/*.png')\n",
        "    n = len(orig_img)\n",
        "    random.shuffle(orig_img)\n",
        "    red_keys=orig_img[:]                                 #redundant keys to avoid key error\n",
        "    train_keys = orig_img[:int(0.9*n*percentage)]\n",
        "    #print(0.9*n*percentage)\n",
        "    #print(len(train_keys))\n",
        "    val_keys = orig_img[-int(0.1*n*percentage):]\n",
        "    #print(len(val_keys))\n",
        "    split_dict = {}\n",
        "    #i=0\n",
        "    for key in red_keys:\n",
        "      split_dict[key]='red'\n",
        "      #i+=1\n",
        "    #print(i)\n",
        "    for key in train_keys:\n",
        "      split_dict[key] = 'train'\n",
        "    for key in val_keys:\n",
        "      split_dict[key] = 'val'\n",
        "      \n",
        "    hazy_img = glob.glob(hazy_img_path + '/*.png')\n",
        "    for img in hazy_img:\n",
        "      \n",
        "      img_name = img.split('/')[-1][:-4].split('_')[0]\n",
        "      orig_path = orig_img_path + '/' + img_name+'.png'\n",
        "      if (split_dict[orig_path] == 'train'):\n",
        "        train_img.append([img, orig_path])\n",
        "      if (split_dict[orig_path] == 'val'):\n",
        "        val_img.append([img, orig_path])\n",
        "      \n",
        "\n",
        "    return train_img, val_img\n",
        "\n",
        "\n",
        "def gen_dataset(ar):\n",
        "  '''\n",
        "  parameters\n",
        "  list of paths\n",
        "  return\n",
        "  list with gt attached \n",
        "  '''\n",
        "  orig_path='/content/drive/MyDrive/kitti/gt'\n",
        "  haze_pth='/content/drive/MyDrive/kitti/hazy'\n",
        "  lst=[]\n",
        "  for i in ar:\n",
        "    name=i.split('/')[-1].split('_')[0]\n",
        "    pthlist=[i,orig_path+'/'+name+'.jpg']\n",
        "    lst.append(pthlist)\n",
        "  return lst\n",
        "\n",
        "def data_path(orig_img_path = '/content/drive/MyDrive/kitti/gt', hazy_img_path = '/content/drive/MyDrive/kitti/hazy'):\n",
        "  \n",
        "  (a,b,c)=dataset_preposses(orig_path=orig_img_path,haze_path=hazy_img_path)\n",
        "  a=gen_dataset(a)\n",
        "  b=gen_dataset(b)\n",
        "  return a,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgY93AX5fNF0"
      },
      "source": [
        "def dataloader(train_data, val_data, batch_size):\n",
        "    print(len(train_data))\n",
        "    train_data_orig = tf.data.Dataset.from_tensor_slices([img[1] for img in train_data]).map(lambda x: load_image(x))\n",
        "    train_data_haze = tf.data.Dataset.from_tensor_slices([img[0] for img in train_data]).map(lambda x: load_image(x))\n",
        "    train = tf.data.Dataset.zip((train_data_haze, train_data_orig)).shuffle(buffer_size=100).batch(batch_size)\n",
        "    \n",
        "    val_data_orig = tf.data.Dataset.from_tensor_slices([img[1] for img in val_data]).map(lambda x: load_image(x))\n",
        "    val_data_haze = tf.data.Dataset.from_tensor_slices([img[0] for img in val_data]).map(lambda x: load_image(x))\n",
        "    val = tf.data.Dataset.zip((val_data_haze, val_data_orig)).shuffle(buffer_size=100).batch(batch_size)\n",
        "    \n",
        "    return train, val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ip7ZWiOfRqC"
      },
      "source": [
        "def display_img(model, hazy_img, orig_img):\n",
        "    \n",
        "    dehazed_img = model(hazy_img,0,training = False)\n",
        "    plt.figure(figsize = (15,15))\n",
        "    \n",
        "    display_list = [hazy_img[0], orig_img[0], dehazed_img[0]]\n",
        "    title = ['Hazy Image', 'Ground Truth', 'Dehazed Image']\n",
        "    \n",
        "    for i in range(3):\n",
        "        plt.subplot(3, 1, i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(display_list[i])\n",
        "        plt.axis('off')\n",
        "        directory='/content/drive/MyDrive/Test/padCheck'\n",
        "        os.chdir(directory)\n",
        "        filename=str(i)+'.jpg'\n",
        "        tf.keras.preprocessing.image.save_img(filename,display_list[i],)\n",
        "        #plt.imsave('plt'+filename,display_list[i])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg3UFlECfxnq"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOEIJHPTf3ZH"
      },
      "source": [
        "def LCAnet():\n",
        "    \n",
        "    inputs = tf.keras.Input(shape = [352,1216, 3])     # height, width of input image changed because of error in output\n",
        "    conv = Conv2D(filters = 50, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',)(inputs)\n",
        "    poolLayer=AveragePooling2D(pool_size=(2,2))(conv)\n",
        "    conv1 = Conv2D(filters = 50, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(poolLayer)\n",
        "    poolLayer=AveragePooling2D(pool_size=(2,2))(conv1)  \n",
        "    #flat=Flatten()(poolLayer)\n",
        "    dens1=Dense(10,activation='relu')(poolLayer)\n",
        "    dens2=Dense(10,activation='relu')(dens1)\n",
        "    deconv1=Conv2DTranspose(50,kernel_size=(3,3),padding='same',activation='relu')(dens2)\n",
        "    upsamp1=UpSampling2D(size=(2,2))(deconv1)\n",
        "    deconv2=Conv2DTranspose(50,kernel_size=(3,3),padding='same',activation='relu')(upsamp1)\n",
        "    upsamp2=UpSampling2D(size=(2,2))(deconv2)\n",
        "    deconv3=Conv2DTranspose(3,kernel_size=(3,3),padding='same',activation='linear')(upsamp2)\n",
        "    output = deconv3\n",
        "    \n",
        "    return Model(inputs = inputs, outputs = output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFuGFJT4f4El"
      },
      "source": [
        "model=LCAnet()\n",
        "model.build([352,1216,3])\n",
        "model.summary()\n",
        "\n",
        "dot_img_file = '/tmp/model_1.png'\n",
        "tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEtT_tFVfSTS"
      },
      "source": [
        "# Hyperparameters\n",
        "epochs = 10\n",
        "\n",
        "k_init = tf.keras.initializers.random_normal(stddev=0.008, seed = 101)      \n",
        "regularizer = tf.keras.regularizers.L2(1e-4)\n",
        "b_init = tf.constant_initializer()\n",
        "batch_size=2\n",
        "train_data, val_data = dataset_preposses(orig_img_path = '/content/drive/MyDrive/kitti/gt', hazy_img_path = '/content/drive/MyDrive/kitti/hazy',percentage=1)\n",
        "train, val = dataloader(train_data, val_data, batch_size)\n",
        "#net = Unsuper_net()\n",
        "optimizer = Adam(learning_rate = 1e-5)\n",
        "train_loss_tracker = tf.keras.metrics.MeanSquaredError(name = \"train loss\")\n",
        "val_loss_tracker = tf.keras.metrics.MeanSquaredError(name = \"val loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbssEcXDfXQi"
      },
      "source": [
        "def train_model(epochs, train, val,net,train_loss_tracker,val_loss_tracker, optimizer):\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        print(\"\\nStart of epoch %d\" % (epoch,), end=' ')\n",
        "        start_time_epoch = time.time()\n",
        "        start_time_step = time.time()\n",
        "        \n",
        "        # training loop\n",
        "        \n",
        "        for step, (train_batch_haze, train_batch_orig) in enumerate(train):\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "\n",
        "                #train_logits = net(train_batch_haze,train_batch_orig,training = True)\n",
        "                #t=t_net(train_batch_haze)\n",
        "                #a=a_net(train_batch_haze)\n",
        "                #out=tf.add(tf.multiply(train_batch_orig,t),tf.multiply(a,(1-t)))\n",
        "                train_logits=net(train_batch_haze,training=True)\n",
        "                #loss = mean_squared_error(train_batch_haze,train_logits)\n",
        "                loss = mean_squared_error(train_batch_orig,train_logits)\n",
        "                #loss=tf.math.reduce_mean(loss,axis=None)\n",
        "                #er=tf.keras.losses.MeanAbsoluteError(tf.keras.losses.Reduction.AUTO)\n",
        "                #loss=er(train_batch_orig, train_logits)/batch_size\n",
        "                #loss=tf.image.psnr(train_batch_orig, train_logits, max_val=1)\n",
        "                #loss=tf.image.ssim(train_batch_orig, train_logits, max_val=1, filter_size=11, filter_sigma=1.5, k1=0.01, k2=0.03)\n",
        "                #loss=custom_loss(train_batch_haze,out)\n",
        "                #loss=tf.math.reduce_mean(loss)\n",
        "                #print(loss)\n",
        "\n",
        "            grads = tape.gradient(loss,net.trainable_weights)\n",
        "            #grads2 = tape.gradient(loss,a_net.trainable_weights)\n",
        "            #optimizer.apply_gradients(zip(grads, t_net.trainable_weights))\n",
        "            optimizer.apply_gradients(zip(grads, net.trainable_weights))\n",
        "\n",
        "            train_loss_tracker.update_state(train_batch_orig, train_logits)\n",
        "            if step == 0:\n",
        "                print('[', end='')\n",
        "            if step % 256 == 0:\n",
        "                print('=', end='')\n",
        "        \n",
        "        print(']', end='')\n",
        "        print('  -  ', end='')\n",
        "        print('Training Loss: %.4f' % (train_loss_tracker.result()), end='')\n",
        "        \n",
        "        # validation loop\n",
        "        \n",
        "        for step, (val_batch_haze, val_batch_orig) in enumerate(val):\n",
        "            val_logits = net(val_batch_haze,0,False)\n",
        "            val_loss_tracker.update_state(val_batch_orig, val_logits)\n",
        "            \n",
        "            if step % 256 ==0:\n",
        "                display_img(net, val_batch_haze, val_batch_orig)\n",
        "        \n",
        "        print('  -  ', end='')\n",
        "        print('Validation Loss: %.4f' % (val_loss_tracker.result()), end='')\n",
        "        print('  -  ', end=' ')\n",
        "        print(\"Time taken: %.2fs\" % (time.time() - start_time_epoch))\n",
        "        \n",
        "        #net.save('trained_model')           # save the model(variables, weights, etc)\n",
        "        train_loss_tracker.reset_states()\n",
        "        val_loss_tracker.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqfplbY8fbHV"
      },
      "source": [
        "%%time\n",
        "train_model(1, train, val,model,train_loss_tracker, val_loss_tracker, tf.keras.optimizers.SGD(learning_rate = 1e-13))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpol46wffd6L"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_gen(net):\n",
        "    \n",
        "    #test_img = glob.glob(test_img_path +'/*.jpg')\n",
        "    test_img=glob.glob('/content/drive/MyDrive/test_set/hazy/*.png')\n",
        "    #random.shuffle(test_img)\n",
        "    i=0;\n",
        "    for img in test_img:\n",
        "        \n",
        "        img = tf.io.read_file(img)\n",
        "        img = tf.io.decode_png(img, channels = 3)\n",
        "        \n",
        "        img = tf.image.resize(img, size = (352,1216), antialias = True)\n",
        "        \n",
        "        img = img / 255.0\n",
        "        print(i,end=\" \")\n",
        "        img = tf.expand_dims(img, axis = 0)      #transform input image from 3D to 4D ###\n",
        "        \n",
        "        dehaze = net(img)\n",
        "        \n",
        "        #plt.figure(figsize = (80, 80))\n",
        "        \n",
        "        #display_list = [img[0], dehaze[0]]       #make the first dimension zero\n",
        "        im=dehaze[0]\n",
        "        #im=((dehaze[0]-0.4)/0.2)*255\n",
        "        #print(im.numpy().min())\n",
        "        directory = '/content/drive/MyDrive/test_kitti'\n",
        "        os.chdir(directory)\n",
        "        filename = str(i) + '_outdoor_gen.png'\n",
        "        #print(filename)\n",
        "        #cv2.imwrite(filename,im) \n",
        "        #plt.imsave(filename,im)\n",
        "        tf.keras.preprocessing.image.save_img(\n",
        "    filename, im)\n",
        "\n",
        "        os.chdir('/content')\n",
        "        i+=1;\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2bKPwAWffb5"
      },
      "source": [
        "evaluate_gen(model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}