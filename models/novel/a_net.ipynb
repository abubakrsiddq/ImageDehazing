{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kitti_anet_10may.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhxbgQ2Ahtb2H7eq3TdC9F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abubakrsiddq/ImageDehazing/blob/main/models/novel/a_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j2Mx_X6Kjml",
        "outputId": "c20fb74d-c81e-4f24-aa4d-48e398533129"
      },
      "source": [
        "!pip install tensorflow_addons\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "import time\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import mean_squared_error\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\r\u001b[K     |▌                               | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 31.3MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30kB 22.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40kB 17.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51kB 10.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 61kB 11.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 71kB 11.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 81kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 92kB 11.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 102kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 112kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 122kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 133kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 143kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 153kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 163kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 174kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 184kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 194kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 204kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 215kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 225kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 235kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 245kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 256kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 266kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 276kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 286kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 296kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 307kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 317kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 327kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 337kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 348kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 358kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 368kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 378kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 389kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 399kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 409kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 419kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 430kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 440kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 450kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 460kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 471kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 481kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 491kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 501kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 512kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 522kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 532kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 542kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 552kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 563kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 573kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 583kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 593kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 604kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 614kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 624kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 634kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 645kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 655kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 665kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 675kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 686kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 696kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 706kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.12.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmET_qcoKk1W",
        "outputId": "ce4818d6-9306-4c12-b5dc-f26b57370405"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToQQMyBfK48y"
      },
      "source": [
        "def load_image(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    #img = tf.io.decode_jpeg(img, channels = 3)\n",
        "    img = tf.io.decode_png(img, channels = 3)\n",
        "    img = tf.image.resize(img, size = (352, 1216), antialias = True)\n",
        "    img = img / 255.0\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtXC4lGNLjF3"
      },
      "source": [
        "def load_airlight(img_path):\n",
        "  air=img_path.split(\"_\")[-1][:-4];\n",
        "  return air"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD7gt2hALmKG"
      },
      "source": [
        "def display_img(model, hazy_img, orig_img):\n",
        "    pred = model(hazy_img)\n",
        "    plt.figure(figsize = (15,15))\n",
        "    \n",
        "    display_list = [hazy_img[0], orig_img[0], pred[0]]\n",
        "    title = ['Hazy Image', 'Ground Truth', 'Dehazed Image']\n",
        "    \n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(display_list[i])\n",
        "        plt.axis('off')\n",
        "        \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVEhNFXILn9G"
      },
      "source": [
        "def data_path(hazy_img_path,percentage=0.8,seed_val=101):\n",
        "    \n",
        "    random.seed(seed_val)\n",
        "    train_img = []\n",
        "    val_img = []\n",
        "    train_air= []\n",
        "    val_air= []\n",
        "    \n",
        "    \n",
        "    #hazy_img = glob.glob(hazy_img_path + '/*.jpg')\n",
        "    hazy_img = glob.glob(hazy_img_path + '/*.png')\n",
        "    n = len(hazy_img)\n",
        "    n1=percentage*n;\n",
        "\n",
        "    random.shuffle(hazy_img)\n",
        "    n=n1;\n",
        "    train_keys = hazy_img[:int(0.9*n)]        #90% data for train, 10% for test\n",
        "    val_keys = hazy_img[int(0.9*n):int(1.0*n)]\n",
        "    #hazy_img=hazy_img[:n]\n",
        "    split_dict = {}\n",
        "    for key in train_keys:\n",
        "        split_dict[key] = 'train'\n",
        "    for key in val_keys:\n",
        "        split_dict[key] = 'val'\n",
        "        \n",
        "    \n",
        "    for img in hazy_img:\n",
        "        air=img.split(\"_\")[-1][:-4];\n",
        "  \n",
        "        try:\n",
        "          if (split_dict[img] == 'train'):\n",
        "              train_air.append(float(air))\n",
        "              train_img.append(img)\n",
        "          else:\n",
        "              val_air.append(float(air))\n",
        "              val_img.append(img)\n",
        "        except KeyError:\n",
        "          pass\n",
        "    return train_img,train_air, val_img,val_air"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8viiKUeGLpdO"
      },
      "source": [
        "def dataloader(train_data,train_res, val_data,val_res, batch_size):\n",
        "    print(len(train_data))\n",
        "    train_data_haze = tf.data.Dataset.from_tensor_slices([img for img in train_data]).map(lambda x: load_image(x))\n",
        "    train_data_airlight = tf.data.Dataset.from_tensor_slices([a for a in train_res])\n",
        "    train = tf.data.Dataset.zip((train_data_haze,train_data_airlight)).shuffle(100).batch(batch_size)\n",
        "\n",
        "    val_data_haze = tf.data.Dataset.from_tensor_slices([img for img in val_data]).map(lambda x: load_image(x))\n",
        "    val_data_airlight = tf.data.Dataset.from_tensor_slices([a for a in val_res])\n",
        "    val = tf.data.Dataset.zip((val_data_haze,val_data_airlight)).shuffle(100).batch(batch_size)\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    return train, val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmOtfz8-LrCm"
      },
      "source": [
        "def build_model(input_shape):\n",
        "    \"\"\"Generates CNN model\n",
        "    :param input_shape (tuple): Shape of input set\n",
        "    :return model: CNN model\n",
        "    \"\"\"\n",
        "\n",
        "    # build network topology\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 1st conv layer\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # 2nd conv layer\n",
        "    model.add(keras.layers.Conv2D(16, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # 3rd conv layer\n",
        "    model.add(keras.layers.Conv2D(8, (2, 2), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # flatten output and feed it into dense layer\n",
        "    model.add(keras.layers.Flatten())\n",
        "    \n",
        "    # 1st dense layer\n",
        "    keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "    keras.layers.Dropout(0.6),\n",
        "\n",
        "    # 2nd dense layer\n",
        "    keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "\n",
        "    # 3rd dense layer\n",
        "    keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "\n",
        "\n",
        "    # output layer\n",
        "    keras.layers.Dense(1, activation='relu')\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe63jIbLL4lO",
        "outputId": "12347151-a4f3-4c9d-c104-374f3b600c8a"
      },
      "source": [
        "# Hyperparameters\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "k_init = tf.keras.initializers.random_normal(stddev=0.008, seed = 101)      \n",
        "regularizer = tf.keras.regularizers.L2(1e-4)\n",
        "b_init = tf.constant_initializer()\n",
        "\n",
        "train_data,train_res, val_data,val_res = data_path(hazy_img_path = '/content/drive/MyDrive/kitti/hazy')\n",
        "train, val = dataloader(train_data,train_res, val_data,val_res, batch_size)\n",
        "\n",
        "optimizer = Adam(learning_rate = 1e-5)\n",
        "net = build_model([352, 1216, 3])\n",
        "train_loss_tracker = tf.keras.metrics.MeanSquaredError(name = \"train loss\")\n",
        "val_loss_tracker = tf.keras.metrics.MeanSquaredError(name = \"val loss\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKcWZ0IxMQoX"
      },
      "source": [
        "def train_model(epochs, train, val, net, train_loss_tracker, val_loss_tracker, optimizer):\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        print(\"\\nStart of epoch %d\" % (epoch,), end=' ')\n",
        "        start_time_epoch = time.time()\n",
        "        start_time_step = time.time()\n",
        "\n",
        "        for step, (train_batch_haze, train_res) in enumerate(train):\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "\n",
        "                train_logits = net(train_batch_haze, training = True)\n",
        "                #print(train_logits.shape)\n",
        "                train_res=np.expand_dims(train_res,axis=-1)\n",
        "                loss = mean_squared_error(train_res, train_logits)\n",
        "\n",
        "            grads = tape.gradient(loss, net.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(grads, net.trainable_weights))\n",
        "            \n",
        "            train_loss_tracker.update_state(train_res, train_logits)\n",
        "            if step == 0:\n",
        "                print('[', end='')\n",
        "            if step % 64 == 0:\n",
        "                print('=', end='')\n",
        "        \n",
        "        print(']', end='')\n",
        "        print('  -  ', end='')\n",
        "        print('Training Loss: %.4f' % (train_loss_tracker.result()), end='')\n",
        "        \n",
        "        for step, (val_batch_haze, val_res) in enumerate(val):\n",
        "            val_logits = net(val_batch_haze, training = False)\n",
        "            val_res=np.expand_dims(val_res,axis=-1)\n",
        "            #print(val_logits.shape)\n",
        "            #print(\"---------------------\")\n",
        "            val_loss_tracker.update_state(val_res, val_logits)\n",
        "            #if step % 64 ==0:\n",
        "                #display_img(net, val_batch_haze, val_batch_orig)\n",
        "        print('  -  ', end='')\n",
        "        print('Validation Loss: %.4f' % (val_loss_tracker.result()), end='')\n",
        "        print('  -  ', end=' ')\n",
        "        print(\"Time taken: %.2fs\" % (time.time() - start_time_epoch))\n",
        "        \n",
        "        train_loss_tracker.reset_states()\n",
        "        val_loss_tracker.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCp2ri9rMVfz",
        "outputId": "f180529b-7e0a-42fb-b7e1-1dbba0d55e5e"
      },
      "source": [
        "%%time\n",
        "train_model(10, train, val, net, train_loss_tracker, val_loss_tracker, optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0 [===]  -  Training Loss: 1.6729  -  Validation Loss: 1.1730  -   Time taken: 2144.46s\n",
            "\n",
            "Start of epoch 1 [===]  -  Training Loss: 1.6527  -  Validation Loss: 1.1618  -   Time taken: 173.54s\n",
            "\n",
            "Start of epoch 2 [===]  -  Training Loss: 1.6372  -  Validation Loss: 1.3010  -   Time taken: 171.64s\n",
            "\n",
            "Start of epoch 3 [===]  -  Training Loss: 1.6282  -  Validation Loss: 1.4839  -   Time taken: 171.21s\n",
            "\n",
            "Start of epoch 4 [===]  -  Training Loss: 1.6204  -  Validation Loss: 1.5830  -   Time taken: 171.18s\n",
            "\n",
            "Start of epoch 5 [===]  -  Training Loss: 1.6130  -  Validation Loss: 1.6045  -   Time taken: 171.06s\n",
            "\n",
            "Start of epoch 6 [===]  -  Training Loss: 1.6061  -  Validation Loss: 1.6167  -   Time taken: 171.00s\n",
            "\n",
            "Start of epoch 7 [===]  -  Training Loss: 1.5991  -  Validation Loss: 1.6404  -   Time taken: 171.02s\n",
            "\n",
            "Start of epoch 8 [===]  -  Training Loss: 1.5922  -  Validation Loss: 1.6313  -   Time taken: 170.75s\n",
            "\n",
            "Start of epoch 9 [===]  -  Training Loss: 1.5851  -  Validation Loss: 1.5972  -   Time taken: 170.71s\n",
            "CPU times: user 23min 25s, sys: 39.3 s, total: 24min 4s\n",
            "Wall time: 1h 1min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfT3lnhRN5Qe"
      },
      "source": [
        "optimizer = Adam(learning_rate = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI6N-wetdS5z",
        "outputId": "7f272ebe-fd84-47e3-963b-e39169a2ebd5"
      },
      "source": [
        "%%time\n",
        "train_model(20, train, val, net, train_loss_tracker, val_loss_tracker, optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0 [===]  -  Training Loss: 1.0607  -  Validation Loss: 1.5361  -   Time taken: 172.84s\n",
            "\n",
            "Start of epoch 1 [===]  -  Training Loss: 0.3803  -  Validation Loss: 0.6960  -   Time taken: 172.45s\n",
            "\n",
            "Start of epoch 2 [===]  -  Training Loss: 0.2599  -  Validation Loss: 0.3418  -   Time taken: 172.06s\n",
            "\n",
            "Start of epoch 3 [===]  -  Training Loss: 0.1744  -  Validation Loss: 0.1790  -   Time taken: 171.66s\n",
            "\n",
            "Start of epoch 4 [===]  -  Training Loss: 0.1162  -  Validation Loss: 0.1012  -   Time taken: 172.12s\n",
            "\n",
            "Start of epoch 5 [===]  -  Training Loss: 0.0785  -  Validation Loss: 0.0634  -   Time taken: 172.28s\n",
            "\n",
            "Start of epoch 6 [===]  -  Training Loss: 0.0553  -  Validation Loss: 0.0445  -   Time taken: 171.74s\n",
            "\n",
            "Start of epoch 7 [===]  -  Training Loss: 0.0420  -  Validation Loss: 0.0349  -   Time taken: 171.90s\n",
            "\n",
            "Start of epoch 8 [===]  -  Training Loss: 0.0349  -  Validation Loss: 0.0302  -   Time taken: 171.41s\n",
            "\n",
            "Start of epoch 9 [===]  -  Training Loss: 0.0313  -  Validation Loss: 0.0282  -   Time taken: 171.20s\n",
            "\n",
            "Start of epoch 10 [===]  -  Training Loss: 0.0297  -  Validation Loss: 0.0273  -   Time taken: 171.00s\n",
            "\n",
            "Start of epoch 11 [===]  -  Training Loss: 0.0290  -  Validation Loss: 0.0271  -   Time taken: 171.78s\n",
            "\n",
            "Start of epoch 12 [===]  -  Training Loss: 0.0287  -  Validation Loss: 0.0270  -   Time taken: 170.88s\n",
            "\n",
            "Start of epoch 13 [===]  -  Training Loss: 0.0287  -  Validation Loss: 0.0270  -   Time taken: 170.91s\n",
            "\n",
            "Start of epoch 14 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0270  -   Time taken: 170.71s\n",
            "\n",
            "Start of epoch 15 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0270  -   Time taken: 170.96s\n",
            "\n",
            "Start of epoch 16 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0270  -   Time taken: 170.58s\n",
            "\n",
            "Start of epoch 17 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0270  -   Time taken: 170.63s\n",
            "\n",
            "Start of epoch 18 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 170.53s\n",
            "\n",
            "Start of epoch 19 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 170.69s\n",
            "CPU times: user 46min 29s, sys: 1min 14s, total: 47min 44s\n",
            "Wall time: 57min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRt5EBwRdWEZ"
      },
      "source": [
        "optimizer = Adam(learning_rate = 1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U0ZRgqg0vww",
        "outputId": "20e4e2ce-5e59-41d0-e73a-da0a770ccaa9"
      },
      "source": [
        "%%time\n",
        "train_model(20, train, val, net, train_loss_tracker, val_loss_tracker, optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 172.55s\n",
            "\n",
            "Start of epoch 1 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 171.65s\n",
            "\n",
            "Start of epoch 2 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 171.25s\n",
            "\n",
            "Start of epoch 3 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 171.01s\n",
            "\n",
            "Start of epoch 4 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 172.48s\n",
            "\n",
            "Start of epoch 5 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 171.11s\n",
            "\n",
            "Start of epoch 6 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 171.05s\n",
            "\n",
            "Start of epoch 7 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 170.93s\n",
            "\n",
            "Start of epoch 8 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 172.29s\n",
            "\n",
            "Start of epoch 9 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 170.68s\n",
            "\n",
            "Start of epoch 10 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 170.76s\n",
            "\n",
            "Start of epoch 11 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 171.08s\n",
            "\n",
            "Start of epoch 12 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 171.79s\n",
            "\n",
            "Start of epoch 13 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 170.76s\n",
            "\n",
            "Start of epoch 14 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 171.83s\n",
            "\n",
            "Start of epoch 15 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 170.97s\n",
            "\n",
            "Start of epoch 16 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 170.93s\n",
            "\n",
            "Start of epoch 17 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 171.35s\n",
            "\n",
            "Start of epoch 18 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 171.23s\n",
            "\n",
            "Start of epoch 19 [===]  -  Training Loss: 0.0286  -  Validation Loss: 0.0271  -   Time taken: 170.94s\n",
            "CPU times: user 46min 25s, sys: 1min 16s, total: 47min 41s\n",
            "Wall time: 57min 6s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBNkRjJN0yZX",
        "outputId": "5433c23b-5a40-482e-b5c6-005442fa73c4"
      },
      "source": [
        "net.save('/content/drive/MyDrive/nets/prelim_model/a_net/0.0271')\n",
        "net.save_weights('/content/drive/MyDrive/nets/prelim_model/a_net/weights/0.0271')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/nets/prelim_model/a_net/0.0271/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRHdytLfHmWf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}